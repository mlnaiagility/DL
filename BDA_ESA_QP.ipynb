{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ce06ad9",
   "metadata": {},
   "source": [
    "# PART 1\n",
    "\n",
    "## 1. List primary differences between Hadoop version-2 and Hadoop version-3\n",
    "\n",
    "1. YARN (Yet Another Resource Negotiator):\n",
    "● Hadoop 2 introduced YARN, which separates resource management from job\n",
    "scheduling. It allows various processing frameworks to share cluster\n",
    "resources efficiently.\n",
    "● Hadoop 3 continues to use YARN but with improvements, enabling better\n",
    "resource utilization, enhanced container management, and more flexible\n",
    "scheduling policies.\n",
    "\n",
    "2. Containerization and Docker Support:\n",
    "● Hadoop 3 introduces support for Docker containers, allowing applications to\n",
    "run within isolated containers. This improves resource utilization, isolation,\n",
    "and simplifies deployment.\n",
    "\n",
    "3. Erasure Coding for Data Replication:\n",
    "● Hadoop 3 introduces erasure coding as an alternative to the traditional 3x\n",
    "replication for data durability. Erasure coding reduces storage overhead while\n",
    "maintaining data reliability.\n",
    "4. NodeManager Enhancements and Scalability:\n",
    "● Hadoop 3 enhances NodeManagers (part of YARN), improving their\n",
    "performance and reliability. This contributes to better resource management\n",
    "and scalibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b185cf74",
   "metadata": {},
   "source": [
    "##  2. What is Hive metastore? Can NoSQL Database -HBase can be configured as hive metastore?\n",
    "\n",
    "The Hive Metastore is a critical component of Apache Hive, a data warehousing and\n",
    "SQL-like querying framework built on top of Hadoop. The Metastore is essentially a\n",
    "centralized repository that stores metadata information about Hive tables, databases,\n",
    "partitions, columns, and other structural components\n",
    "it is possible to use HBase as the storage backend for the Hive Metastore. This configuration\n",
    "can offer benefits in terms of scalability and real-time query access to metadata. When\n",
    "HBase is used as the Metastore backend, it means that the metadata about Hive tables,\n",
    "partitions, etc., is stored in HBase tables instead of traditional relational databases like\n",
    "Apache Derby or MySQL.\n",
    "\n",
    "Here are some key points to consider:\n",
    "\n",
    "Scalability: HBase is designed for high scalability and can handle large amounts of\n",
    "metadata efficiently. This can be advantageous in environments with massive\n",
    "amounts of metadata to manage.\n",
    "\n",
    "Real-time Access: HBase provides real-time query capabilities, which means\n",
    "metadata can be accessed more quickly than in traditional relational databases.\n",
    "\n",
    "Complex Workloads: If your Hive use case involves complex queries and a large\n",
    "number of tables or partitions, using HBase can provide performance benefits.\n",
    "\n",
    "Configuration and Setup: Configuring HBase as the Hive Metastore requires proper\n",
    "setup and configuration. You need to ensure compatibility and choose appropriate\n",
    "HBase configurations.\n",
    "\n",
    "Management and Maintenance: HBase requires its own management and\n",
    "maintenance. You'll need to manage HBase clusters and ensure data consistency\n",
    "and reliability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c714d3",
   "metadata": {},
   "source": [
    "## 3. Using an example, depict how MapReduce computes word count\n",
    "\n",
    "Map Phase:\n",
    "\n",
    "● Split the input text into chunks.\n",
    "● For each word in each chunk, create a key-value pair with the word as the\n",
    "key and 1 as the value.\n",
    "Example: Input: \"Hello world This is a sample text Hello MapReduce\" Output\n",
    "(intermediate key-value pairs): \"Hello\" 1 \"world\" 1 \"This\" 1 \"is\" 1 \"a\" 1 \"sample\"1 \"text\"\n",
    "1 \"Hello\" 1 \"MapReduce\" 1\n",
    "\n",
    "Shuffle and Sort:\n",
    "● Group and sort the intermediate pairs by key.\n",
    "Reduce Phase:\n",
    "● For each unique key, add up the values to calculate the word frequency.\n",
    "Example: Input (sorted by key): \"Hello\" [1, 1] \"MapReduce\" [1] \"This\" [1] \"a\" [1] \"is\" [1]\n",
    "\"sample\" [1] \"text\" [1] \"world\" [1]\n",
    "Output (word frequency): \"Hello\" 2 \"MapReduce\" 1 \"This\" 1 \"a\" 1 \"is\" 1 \"sample\" 1\n",
    "\"text\" 1 \"world\" 1\n",
    "\n",
    "In a nutshell, MapReduce processes the input data in two phases: the Map phase where\n",
    "data is transformed into key-value pairs, and the Reduce phase where these pairs are\n",
    "grouped and processed to produce the desired output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131c7242",
   "metadata": {},
   "source": [
    "## 4.Draw Spark architecture and explain its various components\n",
    "\n",
    "Driver Program:\n",
    "● The entry point of a Spark application.\n",
    "● Contains the main function and defines the operations to be executed on the\n",
    "data.\n",
    "● Manages the overall execution and coordinates tasks across the cluster.\n",
    "\n",
    "Cluster Manager:\n",
    "● Manages the resources and nodes in the cluster where Spark runs.\n",
    "● Can be standalone, Apache Hadoop YARN, or Apache Mesos.\n",
    "\n",
    "Cluster Nodes:\n",
    "● Physical or virtual machines that contribute computing resources to the Spark\n",
    "application.\n",
    "● Consist of two types of components: the Driver Node and Worker Nodes.\n",
    "\n",
    "Driver Node:\n",
    "● Runs the driver program and orchestrates the execution.\n",
    "● Communicates with the cluster manager to request resources and manage\n",
    "tasks.\n",
    "\n",
    "Worker Nodes:\n",
    "● Execute tasks assigned by the driver program.\n",
    "● Managed by the cluster manager and perform data processing.\n",
    "Spark Context (SparkSession in newer versions):\n",
    "● The core entry point for Spark functionality.\n",
    "● Represents the connection to a Spark cluster and is responsible for\n",
    "coordinating the execution of tasks.\n",
    "\n",
    "Transformations:\n",
    "● Operations applied to RDDs to create new RDDs.\n",
    "● Examples: map, filter, groupByKey, reduceByKey.\n",
    "Spark Core:\n",
    "○ The foundation of Spark that provides basic functionalities like task\n",
    "scheduling, memory management, and fault recovery\n",
    "\n",
    "Spark SQL:\n",
    "○ A Spark module for structured data processing.\n",
    "○ Allows executing SQL queries alongside Spark programs.\n",
    "○ Supports querying various data sources, including Hive, Parquet,\n",
    "JSON, etc.\n",
    "\n",
    "Spark Streaming:\n",
    "○ Allows processing real-time data streams.\n",
    "○ Supports micro-batch processing and integration with sources like\n",
    "Kafka.\n",
    "\n",
    "GraphX:\n",
    "● A library for graph processing and analysis.\n",
    "● Supports graph computation, pattern matching, and graph algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702b10f3",
   "metadata": {},
   "source": [
    "## 5. What is cap theorem ? Where does MongoDB stands in cap theorem\n",
    "\n",
    "The CAP theorem, also known as Brewer's theorem, is a principle in distributed systems that\n",
    "states that it's impossible for a distributed system to simultaneously provide all three of the\n",
    "following guarantees:\n",
    "Consistency: Every read from the system returns the most recent write or an error. In\n",
    "other words, all nodes in the distributed system have the same data at the same\n",
    "time.\n",
    "Availability: Every request to the system, whether it's a read or a write, receives a\n",
    "response, without a guarantee that it contains the most recent version of the data.\n",
    "Partition Tolerance: The system continues to function even when network partitions\n",
    "occur, i.e., communication between nodes is lost or delayed\n",
    "\n",
    "MongoDB is a NoSQL database that stands somewhere between Consistency and\n",
    "Availability in the CAP theorem. It leans more towards providing Consistency and Availability\n",
    "over strong Partition Tolerance.\n",
    "MongoDB uses a design principle called \"Eventual Consistency,\" which means that\n",
    "the system aims to achieve consistency over time, but it might not provide it instantaneously.\n",
    "MongoDB's architecture is designed to be highly available and to tolerate network partitions,\n",
    "which is aligned with the Availability and Partition Tolerance aspects of the CAP theorem.\n",
    "In scenarios where network partitions or node failures occur, MongoDB might choose\n",
    "to prioritize availability and continue serving read and write requests even if it means\n",
    "temporarily sacrificing strict consistency. This makes MongoDB suitable for applications\n",
    "where high availability is crucial, and minor inconsistencies in data (eventual consistency)\n",
    "can be tolerated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc38e6ba",
   "metadata": {},
   "source": [
    "## 6. Draw Hadoop- 2 (YARN) architecture and explain its various components.\n",
    "\n",
    "ResourceManager (RM):\n",
    "● The central master daemon responsible for overall resource management\n",
    "and scheduling.\n",
    "● Manages cluster resources, allocating resources to different applications\n",
    "based on their requirements.\n",
    "● There is one ResourceManager per cluster.\n",
    "\n",
    "NodeManager (NM):\n",
    "● Each worker machine in the cluster runs a NodeManager.\n",
    "● Responsible for managing resources on that node, monitoring resource\n",
    "usage, and reporting back to the ResourceManager.\n",
    "● Manages containers (execution environments for applications) and their\n",
    "lifecycle.\n",
    "\n",
    "ApplicationMaster (AM):\n",
    "● Each application running on the cluster has its own ApplicationMaster.\n",
    "● Responsible for negotiating resources with the ResourceManager, monitoring\n",
    "the application's progress, and handling task failures and retries.\n",
    "● Coordinates the execution of tasks within the application.\n",
    "\n",
    "Container:\n",
    "● A unit of resource allocation in YARN.\n",
    "● Represents a collection of resources (CPU, memory) on a NodeManager\n",
    "allocated to an application.\n",
    "● It can run one or more tasks.\n",
    "\n",
    "Resource Scheduler:\n",
    "● Part of the ResourceManager, responsible for allocating resources to various\n",
    "applications.\n",
    "● Utilizes policies to determine how resources are allocated based on memory,\n",
    "CPU, and other factors.\n",
    "\n",
    "ApplicationManager (AM) Scheduler:\n",
    "● Part of the ApplicationMaster, responsible for managing resources within an\n",
    "application.\n",
    "● Requests resources from the ResourceManager's scheduler and manages\n",
    "their allocation among tasks.\n",
    "\n",
    "Job History Server:\n",
    "● Stores information about completed applications and their tasks for historical\n",
    "analysis and troubleshooting.\n",
    "● Provides a web interface to access this information.\n",
    "YARN APIs:\n",
    "● YARN provides APIs for developers to submit applications and manage their\n",
    "resources.\n",
    "● This allows for various applications (MapReduce, Spark, Tez, etc.) to run on\n",
    "YARN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df2ed81",
   "metadata": {},
   "source": [
    "## 7. What is bucketing in Hive? List any two benefits of bucketing Table? Write an example to create a Bucketed Table in Hive\n",
    "\n",
    "In Hive, bucketing is a technique used to organize data within a table into a specific number\n",
    "of equal-sized buckets based on the values of one or more columns. Each bucket contains a\n",
    "subset of the data, and the data distribution is determined by the hash value of the bucketing\n",
    "column(s). Bucketing is often used in conjunction with Hive's internal sort mechanism to\n",
    "improve query performance.\n",
    "\n",
    "Benefits of Bucketing Tables in Hive:\n",
    "Improved Query Performance: Bucketing reduces the number of data files to scan\n",
    "during queries. When combined with sorting, it enables more efficient data retrieval,\n",
    "especially when performing joins, aggregations, and sampling.\n",
    "\n",
    "Data Skew Handling: If certain values in the bucketing column have significantly\n",
    "more data than others, bucketing can help distribute the data more evenly across\n",
    "buckets, reducing data skew and improving query performance\n",
    "\n",
    "CREATE TABLE bucketed_user(\n",
    "firstname VARCHAR(64),\n",
    "lastname VARCHAR(64),\n",
    "address STRING,\n",
    "city VARCHAR(64),\n",
    "state VARCHAR(64),\n",
    "post STRING,\n",
    "phone1 VARCHAR(64),\n",
    "phone2 STRING,\n",
    "email STRING,\n",
    "web STRING\n",
    ")\n",
    "COMMENT 'A bucketed sorted user table'\n",
    "PARTITIONED BY (country VARCHAR(64))\n",
    "CLUSTERED BY (state) SORTED BY (city) INTO 32 BUCKETS\n",
    "STORED AS SEQUENCEFILE;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52eb0f3f",
   "metadata": {},
   "source": [
    "## 8. List any 5 differences between batch and stream processing\n",
    "\n",
    "Data Processing Approach:\n",
    "● Batch Processing: In batch processing, data is collected, stored, and\n",
    "processed in predefined batches. The processing is executed at a\n",
    "scheduled interval or when a sufficient amount of data accumulates.\n",
    "● Stream Processing: In stream processing, data is processed in real-time as\n",
    "it arrives, with each data item being processed individually or in small\n",
    "groups.\n",
    "\n",
    "Latency and Speed:\n",
    "● Batch Processing: Typically has higher latency as data is processed in\n",
    "large batches, leading to a delay between data ingestion and processing\n",
    "results.\n",
    "● Stream Processing: Offers low latency, enabling real-time or near-real-time\n",
    "processing and quick response to events as they occur.\n",
    "Processing Complexity:\n",
    "● Batch Processing: Suited for complex and resource-intensive processing\n",
    "tasks that can be optimized for efficiency when working with larger data\n",
    "volumes.\n",
    "● Stream Processing: Often used for simpler and lightweight processing\n",
    "tasks that require quick reactions to events, such as filtering, aggregating,or enriching data.\n",
    "\n",
    "Scalability:\n",
    "● Batch Processing: Generally easier to scale for large volumes of data since\n",
    "processing can be parallelized across batch jobs.\n",
    "● Stream Processing: Scales well for handling data streams from various\n",
    "sources and distributing processing across multiple nodes.\n",
    "\n",
    "Use Cases:\n",
    "● Batch Processing: Commonly used for tasks that do not require immediate\n",
    "responses, such as generating reports, data warehousing, and large-scale\n",
    "data transformations.\n",
    "● Stream Processing: Ideal for scenarios where real-time data analysis is\n",
    "crucial, such as monitoring, fraud detection, recommendation systems and IoT data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77022999",
   "metadata": {},
   "source": [
    "## 9. What is Kafka ? Briefly explain the following 4 Kafka terminologies 1.Kafka Broker 2. Kafka Topic 3. Partitions 4. Replication\n",
    "\n",
    "Apache Kafka is an open-source distributed event streaming platform used for building\n",
    "real-time data pipelines and streaming applications. It is designed to handle\n",
    "high-throughput, fault tolerance, and scalability of data streaming and event processing.\n",
    "Kafka Terminologies:\n",
    "\n",
    "Kafka Broker:\n",
    "● A Kafka cluster consists of multiple Kafka brokers.\n",
    "● A Kafka broker is a server that stores and manages the streams of records\n",
    "(events).\n",
    "● Each broker hosts one or more Kafka topics and manages the read and\n",
    "write operations for those topics.\n",
    "● Brokers work together to form the Kafka cluster, ensuring data replication\n",
    "and high availability.\n",
    "\n",
    "Kafka Topic:\n",
    "● A Kafka topic is a logical channel or category to which records\n",
    "(messages/events) are published.\n",
    "● Topics act as data feeds where producers publish records, and consumers\n",
    "subscribe to consume those records.\n",
    "● Topics are organized within Kafka based on their logical grouping and\n",
    "content\n",
    "\n",
    "Partitions:\n",
    "● A Kafka topic can be split into multiple partitions.\n",
    "● Partitions are the basic unit of parallelism and scalability in Kafka.\n",
    "● Each partition is an ordered, immutable sequence of records and has a\n",
    "unique identifier called a partition key.\n",
    "● Data within a partition is ordered, and Kafka guarantees that records with\n",
    "the same key will always go to the same partition, ensuring consistency.\n",
    "\n",
    "Replication:\n",
    "● Kafka provides data replication for fault tolerance and high availability.\n",
    "● Each partition can have one or more replicas.\n",
    "● Replicas are copies of the partition's data, stored on different brokers.\n",
    "● Replication ensures that if a broker or partition becomes unavailable, the\n",
    "data remains accessible from its replicas, maintaining data durability.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ffdc81",
   "metadata": {},
   "source": [
    "\n",
    "#Section C.\n",
    "\n",
    "Bangalore Housing Dataset is provided and loaded as Spark-DataFrame. Using Spark libraries execute the steps, as questioned below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83be438",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path='Bengaluru_House_Data.csv' \n",
    "\n",
    "## to load the data spark Dataframe use/execute below \n",
    "df=spark.read.format(\"csv\").option(\"header\", \"True\").option(\"inferschema\",\"True\").load(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68767156",
   "metadata": {},
   "source": [
    "\n",
    "3.a. Using PySpark and Spark-SQL libraries process the dataset to find out solutions of queries mentioned below.\n",
    "3.a.(i). Count the total number of housing-properties listed from 'HSR Layout' location. (2 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38aae6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(df.location =='HSR Layout').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72973acf",
   "metadata": {},
   "source": [
    "3.a.(ii). How many ‘2 BHK’ size housing-properties are listed from 'Whitefield' location?( 3 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49e1ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter((df.location =='Whitefield') & (df.size =='2 BHK') ).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c036b7d",
   "metadata": {},
   "source": [
    "3.a.(iii). What is the average price of ‘2 BHK’ size housing-properties in ‘HSR Layout’ location? (5 Marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed3cfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter((df.location =='HSR Layout') & (df.size =='2 BHK') ).agg(avg('price')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439a43cb",
   "metadata": {},
   "source": [
    "3.b. Using SparkML libraries execute the steps, as questioned below, in order to build a PySpark regression ML-model on provided Bangalore Housing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041484e4",
   "metadata": {},
   "source": [
    "3.b.(i). Remove the features, having more than one third of their entries as missing/null. For the remaining missing values- remove the corresponding row entry from the DataFrame. (3 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38d2714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get the feature vise missing value use below code\n",
    "df.select([count(when(isnull(c), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5a1570",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select([count(when(isnull(c), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df35308",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af9851d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove missing values feature\n",
    "df = df.drop('society')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52fb649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing rest of the rows with missing entry\n",
    "df = df.na.drop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb05fe1",
   "metadata": {},
   "source": [
    "3.b.(ii). Convert all string columns into numeric values using StringIndexer transformer and make sure now DataFrame does not have any string columns anymore.(5 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441e5e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "stringIndexer = StringIndexer(inputCols=['area_type','availability','location','size','total_sqft'], \n",
    "               outputCols=['area_type_numeric','availability_numeric','location_numeric','size_numeric','total_sqft_numeric'])\n",
    "    \n",
    "model = stringIndexer.fit(df)\n",
    "df = model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afa4bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('area_type','availability','location','size','total_sqft')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a838395c",
   "metadata": {},
   "source": [
    "3.b.(iii). Using vectorAssembler combines all columns (except target column i.e. 'price') of spark DataFrame into single column (named as features). Make sure DataFrame now contains only two columns features and price.(5 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bd28a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cols = df.columns\n",
    "input_cols.remove('price')\n",
    "input_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad865769",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector= VectorAssembler(inputCols=input_cols, outputCol='features')\n",
    "df= vector.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e6548d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('bath',\n",
    " 'balcony',\n",
    " 'area_type_numeric',\n",
    " 'availability_numeric',\n",
    " 'location_numeric',\n",
    " 'size_numeric',\n",
    " 'total_sqft_numeric')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb9259a",
   "metadata": {},
   "source": [
    "3.b.(iv). Split the vectorised dataframe into training and test sets with one fourth records being held for testing (2 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c3fd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    " df_train, df_test =df.randomSplit([0.75, 0.25])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45597631",
   "metadata": {},
   "source": [
    "3.b.(iv). Train default LinearRegression model with features as 'featuresCol' and ‘price’ as label (5 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e216c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to build the LogisticRegression model refer below \n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"price\")\n",
    "lr.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d56bdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90480ac8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57981da4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19fc064",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60dc1c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/08/18 19:15:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/08/18 19:15:03 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "23/08/18 19:15:03 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "23/08/18 19:15:03 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "23/08/18 19:15:03 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "23/08/18 19:15:03 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "23/08/18 19:15:03 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "23/08/18 19:15:03 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "23/08/18 19:15:03 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "23/08/18 19:15:03 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "23/08/18 19:15:03 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "23/08/18 19:15:03 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "23/08/18 19:15:03 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "23/08/18 19:15:03 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "23/08/18 19:15:03 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "23/08/18 19:15:03 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "23/08/18 19:15:03 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.\n",
      "23/08/18 19:15:03 ERROR SparkContext: Error initializing SparkContext.\n",
      "java.net.BindException: Can't assign requested address: Service 'sparkDriver' failed after 16 retries (on a random free port)! Consider explicitly setting the appropriate binding address for the service 'sparkDriver' (for example spark.driver.bindAddress for SparkDriver) to the correct binding address.\n",
      "\tat java.base/sun.nio.ch.Net.bind0(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.bind(Net.java:555)\n",
      "\tat java.base/sun.nio.ch.ServerSocketChannelImpl.netBind(ServerSocketChannelImpl.java:337)\n",
      "\tat java.base/sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:294)\n",
      "\tat io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:141)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:562)\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1334)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:600)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:579)\n",
      "\tat io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:973)\n",
      "\tat io.netty.channel.AbstractChannel.bind(AbstractChannel.java:260)\n",
      "\tat io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:356)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:174)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:167)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.net.BindException: Can't assign requested address: Service 'sparkDriver' failed after 16 retries (on a random free port)! Consider explicitly setting the appropriate binding address for the service 'sparkDriver' (for example spark.driver.bindAddress for SparkDriver) to the correct binding address.\n\tat java.base/sun.nio.ch.Net.bind0(Native Method)\n\tat java.base/sun.nio.ch.Net.bind(Net.java:555)\n\tat java.base/sun.nio.ch.ServerSocketChannelImpl.netBind(ServerSocketChannelImpl.java:337)\n\tat java.base/sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:294)\n\tat io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:141)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:562)\n\tat io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1334)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:600)\n\tat io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:579)\n\tat io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:973)\n\tat io.netty.channel.AbstractChannel.bind(AbstractChannel.java:260)\n\tat io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:356)\n\tat io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:174)\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:167)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/7h/2lb3lpps1sz3jwlsf91qjdyc0000gn/T/ipykernel_1980/1519354811.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRegressionEvaluator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m             self._do_init(\n\u001b[0m\u001b[1;32m    201\u001b[0m                 \u001b[0mmaster\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m                 \u001b[0mappName\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_do_init\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;31m# Create the Java SparkContext through Py4J\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjsc\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m         \u001b[0;31m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_initialize_context\u001b[0;34m(self, jconf)\u001b[0m\n\u001b[1;32m    415\u001b[0m         \"\"\"\n\u001b[1;32m    416\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJavaSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1586\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1587\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1588\u001b[0m             answer, self._gateway_client, None, self._fqn)\n\u001b[1;32m   1589\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.net.BindException: Can't assign requested address: Service 'sparkDriver' failed after 16 retries (on a random free port)! Consider explicitly setting the appropriate binding address for the service 'sparkDriver' (for example spark.driver.bindAddress for SparkDriver) to the correct binding address.\n\tat java.base/sun.nio.ch.Net.bind0(Native Method)\n\tat java.base/sun.nio.ch.Net.bind(Net.java:555)\n\tat java.base/sun.nio.ch.ServerSocketChannelImpl.netBind(ServerSocketChannelImpl.java:337)\n\tat java.base/sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:294)\n\tat io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:141)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:562)\n\tat io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1334)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:600)\n\tat io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:579)\n\tat io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:973)\n\tat io.netty.channel.AbstractChannel.bind(AbstractChannel.java:260)\n\tat io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:356)\n\tat io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:174)\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:167)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n"
     ]
    }
   ],
   "source": [
    "# import statements\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.functions import isnull, when, count, col,avg\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    " \n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476a2d84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
